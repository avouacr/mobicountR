# ----------------------------------------------------------
# Merging CANCAN data and FluxVision cells coverage maps
# ----------------------------------------------------------
# 13/03/19

# ------------------------------------------------------------------------------------------------------------
# spark-submit --executor-memory 16G --num-executors 80 --executor-cores 6 --queue HQ_OLPS --conf 'spark.default.parallelism=8000' --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' --conf 'spark.sql.shuffle.partitions=8000' --conf 'spark.executorEnv.PYTHONHASHSEED=321' --conf 'spark.sql.broadcastTimeout=36000' script.py
# ------------------------------------------------------------------------------------------------------------
# from pyspark.conf import SparkConf
# from pyspark.sql import SparkSession
# conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '8g'), ('spark.num.executors', '100'), ('spark.executor.cores', '4'), ('spark.default.parallelism', '100'), ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'), ('spark.sql.shuffle.partitions', '100'), ('spark.executorEnv.PYTHONHASHSEED', '321'), ('spark.sql.broadcastTimeout', '36000')])
# spark = SparkSession.builder.config(conf=conf).appName("Home Detection").getOrCreate()
# sc = spark.sparkContext
# ------------------------------------------------------------------------------------------------------------


# ============================
# Parametrage: Spark 2.1


# MODULES
import sys
from pyspark.sql.functions import (isnull, isnan, when, count, col, expr, 
format_number, lit, sum, desc, concat, month, dayofmonth, hour)
from pyspark.sql.types import (StructType, StructField, IntegerType, 
StringType, TimestampType, DoubleType)
from pyspark.sql import SparkSession, Row

spark = SparkSession.builder.appName("Test appariement 4G").getOrCreate()
sc = spark.sparkContext

hue_perso = "/User/zues5490/WORK/romain/"
hue_cancan = "/Data/P17100/SECURE/RAW/TXT/"


# FV coverage maps : conversion from .csv to parquet --------------------------

schema_fv = StructType([
	    StructField("dma_name", StringType(), True),
	    StructField("x_tile", DoubleType(), True),
	    StructField("y_tile", DoubleType(), True),
	    StructField("proba", DoubleType(), True)]
	    )

fv_indoor = (spark.read
.option("mode", "DROPMALFORMED") # Option to drop first line (wrongly formatted)
.schema(schema_fv)
.csv(hue_perso + "fv_indoor_07_18.tar.bz2"))

(fv_indoor.write
.option("header","false")
.parquet(hue_perso + "fv_indoor_07_18_pqt"))

